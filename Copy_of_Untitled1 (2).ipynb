{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lToePHjXbGB1"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "#Keras imports\r\n",
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, AvgPool2D, Flatten, Dense\r\n",
        "from keras.optimizers import Adam \r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_MbntNNZnZZ"
      },
      "source": [
        "number_of_dataset_classes = 10\r\n",
        "number_of_K_folds = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SICsz20Rksv"
      },
      "source": [
        "def get_dataset(dataset_name):\r\n",
        "  if dataset_name == 'mnist':\r\n",
        "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
        "  elif dataset_name == 'fasnion_minst':\r\n",
        "    pass\r\n",
        "  return (X_train, Y_train), (X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka5e_LGfY_lD"
      },
      "source": [
        "def separate_dataset_into_K_folds(X_train, Y_train, number_of_K_folds):\r\n",
        "  if number_of_K_folds == 10:\r\n",
        "    folds = get_10_folds(X_train, Y_train)\r\n",
        "  elif number_of_K_folds == 5:\r\n",
        "    folds = get_5_folds(X_train, Y_train)\r\n",
        "  return folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW0wc7VoaRai"
      },
      "source": [
        "def get_10_folds(X_train, Y_train, number_of_dataset_classes = number_of_dataset_classes):\r\n",
        "  dataset_classes = get_dataset_classes(X_train, Y_train, number_of_dataset_classes)\r\n",
        "  X_folds = [[],[],[],[],[],[],[],[],[],[]]\r\n",
        "  Y_folds = [[],[],[],[],[],[],[],[],[],[]]\r\n",
        "  \r\n",
        "  #Pick each dataset class\r\n",
        "  for jj, dataset_class in enumerate(dataset_classes):\r\n",
        "    image_index = 0\r\n",
        "    while image_index < len(dataset_class):\r\n",
        "        for ii, fold in enumerate(X_folds):\r\n",
        "          try:\r\n",
        "            #print('image index is: ', image_index, 'dataset_class_index is: ', dataset_class_index)\r\n",
        "            X_folds[ii].append(dataset_class[image_index])\r\n",
        "            Y_folds[ii].append(jj)\r\n",
        "            image_index += 1\r\n",
        "          except Exception as e:\r\n",
        "            continue\r\n",
        "        \r\n",
        "  #Convert X_folds and Y_folds to numpy arrays\r\n",
        "  for ii, fold in enumerate(X_folds):\r\n",
        "    X_folds[ii] = np.array(fold)\r\n",
        "  for ii, fold in enumerate(Y_folds):\r\n",
        "    Y_folds[ii] = np.array(fold)\r\n",
        "  X_folds = np.array(X_folds)\r\n",
        "  Y_folds = np.array(Y_folds)\r\n",
        "\r\n",
        "  for ii, X_fold in enumerate(X_folds):\r\n",
        "    Y_fold = Y_folds[ii]\r\n",
        "    #c = np.array([X_fold, Y_fold])\r\n",
        "    indices = np.arange(X_fold.shape[0])\r\n",
        "    np.random.shuffle(indices)\r\n",
        "    # c[0] = c[0][indices]\r\n",
        "    # c[1] = c[1][indices]\r\n",
        "    # X_fold, Y_fold = c[0], c[1]\r\n",
        "    X_fold = X_fold[indices]\r\n",
        "    Y_fold = Y_fold[indices]\r\n",
        "    X_folds[ii] = X_fold\r\n",
        "    Y_folds[ii] = Y_fold\r\n",
        "\r\n",
        "  return X_folds, Y_folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjJ6daV0Pfu8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzw9kLSFbKIn"
      },
      "source": [
        "def get_dataset_classes(X_train, Y_train, number_of_dataset_classes):\r\n",
        "  if number_of_dataset_classes == 10:\r\n",
        "    dataset_classes = [[],[],[],[],[],[],[],[],[],[]]\r\n",
        "  elif number_of_dataset_classes == 5:\r\n",
        "    dataset_classes = [[],[],[],[],[]]\r\n",
        "  for dataset_class_index in range(number_of_dataset_classes):\r\n",
        "    for item in range(X_train.shape[0]):\r\n",
        "      if Y_train[item] == dataset_class_index:\r\n",
        "        dataset_classes[dataset_class_index].append(X_train[item])\r\n",
        "  return np.array(dataset_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgevJCFke7Xp"
      },
      "source": [
        "def create_fold_iterables(X_folds, Y_folds):\r\n",
        "  iterables = []\r\n",
        "  for ii, val_fold in enumerate(X_folds):\r\n",
        "    X_stack = 0\r\n",
        "    Y_stack = 0\r\n",
        "    for jj, train_fold in enumerate(X_folds):\r\n",
        "      if ii != jj:\r\n",
        "        if type(X_stack) is int:\r\n",
        "          X_stack = train_fold\r\n",
        "          Y_stack = Y_folds[jj]\r\n",
        "        else:\r\n",
        "          X_stack= np.vstack((X_stack, train_fold))\r\n",
        "          Y_stack= np.hstack((Y_stack, Y_folds[jj]))\r\n",
        "\r\n",
        "    iterables.append([X_stack, Y_stack, val_fold, Y_folds[ii]])\r\n",
        "  iterables = np.array(iterables)\r\n",
        "  return iterables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcty3QInZPh_"
      },
      "source": [
        "#https://medium.com/towards-artificial-intelligence/the-architecture-implementation-of-lenet-5-eef03a68d1f7\r\n",
        "def create_lenet5_model_with_1_conv_layers():\r\n",
        "\r\n",
        "  # Instanciate an empty model\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  # Adding a Convolution Layer C1\r\n",
        "  # Input shape = N = (28 x 28)\r\n",
        "  # No. of filters  = 6\r\n",
        "  # Filter size = f = (5 x 5)\r\n",
        "  # Padding = P = 0\r\n",
        "  # Strides = S = 1\r\n",
        "  # Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24\r\n",
        "  # No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156\r\n",
        "  model.add(Conv2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=(28,28,1), activation='tanh'))\r\n",
        "\r\n",
        "  # Adding an Average Pooling Layer S2\r\n",
        "  # Input shape = N = (24 x 24)\r\n",
        "  # No. of filters = 6\r\n",
        "  # Filter size = f = (2 x 2)\r\n",
        "  # Padding = P = 0\r\n",
        "  # Strides = S = 2\r\n",
        "  # Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12\r\n",
        "  # No. of parameters between C1 and S2 = (1+1)*6 = 12\r\n",
        "  model.add(AvgPool2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "  # Adding a Convolution Layer C3\r\n",
        "  # Input shape = N = (12 x 12)\r\n",
        "  # No. of filters  = 16\r\n",
        "  # Filter size = f = (5 x 5)\r\n",
        "  # Padding = P = 0\r\n",
        "  # Strides = S = 1\r\n",
        "  # Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8\r\n",
        "  # No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416\r\n",
        "  model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\r\n",
        "\r\n",
        "  # Adding an Average Pooling Layer S4\r\n",
        "  # Input shape = N = (8 x 8)\r\n",
        "  # No. of filters = 16\r\n",
        "  # Filter size = f = (2 x 2)\r\n",
        "  # Padding = P = 0\r\n",
        "  # Strides = S = 2\r\n",
        "  # Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4\r\n",
        "  # No. of parameters between C3 and S4 = (1+1)*16 = 32\r\n",
        "  model.add(AvgPool2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "  # As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of \r\n",
        "  # convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more\r\n",
        "  # convolution here.\r\n",
        "\r\n",
        "  # Flattening the layer S4\r\n",
        "  # There would be 16*(4*4) = 256 neurons\r\n",
        "  model.add(Flatten())\r\n",
        "\r\n",
        "  # Adding a Dense layer with `tanh` activation+# \r\n",
        "  # No. of inputs = 256\r\n",
        "  # No. of outputs = 120\r\n",
        "  # No. of parameters = 256*120 + 120 = 30,840\r\n",
        "  model.add(Dense(120, activation='tanh'))\r\n",
        "\r\n",
        "  # Adding a Dense layer with `tanh` activation\r\n",
        "  # No. of inputs = 120\r\n",
        "  # No. of outputs = 84\r\n",
        "  # No. of parameters = 120*84 + 84 = 10,164\r\n",
        "  model.add(Dense(84, activation='tanh'))\r\n",
        "\r\n",
        "  # Adding a Dense layer with `softmax` activation\r\n",
        "  # No. of inputs = 84\r\n",
        "  # No. of outputs = 10\r\n",
        "  # No. of parameters = 84*10 + 10 = 850\r\n",
        "  model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "  #model.summary()\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FyhtcyvAuS2"
      },
      "source": [
        "def compile_and_fit_model(model, train_x, train_y):#, val_x, val_y):\r\n",
        "  #Reshape data\r\n",
        "  train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\r\n",
        "  #val_x = val_x.reshape(val_x.shape[0], 28, 28, 1)\r\n",
        "\r\n",
        "  #Normalize data\r\n",
        "  train_x = train_x/255.0\r\n",
        "  #val_x = val_x/255.0\r\n",
        "\r\n",
        "  #One-hot encode the labels\r\n",
        "  train_y = to_categorical(train_y, num_classes=10)\r\n",
        "  #print('train_y.shape is: ', train_y.shape)\r\n",
        "  #val_y = to_categorical(val_y, num_classes=10)\r\n",
        "\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\r\n",
        "  model.fit(train_x, train_y, batch_size=128, epochs=20, verbose=0)#, validation_data=(val_x, val_y))\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGIMJOw0A5bN"
      },
      "source": [
        "def evaluate_model(model, test_x, test_y):\r\n",
        "  #val_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\r\n",
        "  print('val_x.shape: ', val_x.shape)\r\n",
        "  val_x = test_x/255.0\r\n",
        "  val_y = to_categorical(test_y, num_classes=10)\r\n",
        "  print('val_y.shape: ', val_y.shape)\r\n",
        "  score = model.evaluate(val_x, val_y, batch_size=128)\r\n",
        "  print('Test Loss:', score[0])\r\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_e_nZYpYQPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1ab14c-27e1-4b82-c011-114b5b2bf125"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = get_dataset('mnist')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh36RHoKWD-h",
        "outputId": "85f4d563-1d01-4d03-ff2e-8c751b50e14f"
      },
      "source": [
        "X_folds, Y_folds = separate_dataset_into_K_folds(X_train, Y_train, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVOTNPBUUt3v",
        "outputId": "26e78237-b4e2-420f-f776-e33ae9a11aeb"
      },
      "source": [
        "iterables = create_fold_iterables(X_folds, Y_folds)\r\n",
        "scores = []\r\n",
        "for iterable in tqdm(iterables):\r\n",
        "  K_fold_X_train, K_fold_Y_train, K_fold_X_test, K_fold_Y_test = iterable\r\n",
        "  # print(K_fold_X_train.shape)\r\n",
        "  # print(K_fold_Y_train.shape)\r\n",
        "  # print(K_fold_X_test.shape)\r\n",
        "  # print(K_fold_Y_test.shape)\r\n",
        "  val_x = K_fold_X_test.reshape(K_fold_X_test.shape[0], 28, 28, 1)\r\n",
        "  val_y = to_categorical(K_fold_Y_test, num_classes=10)\r\n",
        "  model = create_lenet5_model_with_1_conv_layers()\r\n",
        "  model = compile_and_fit_model(model, K_fold_X_train, K_fold_Y_train)\r\n",
        "  #evaluate_model(model, K_fold_X_test, K_fold_Y_test)\r\n",
        "  score = model.evaluate(val_x, val_y, verbose=0)\r\n",
        "  print('\\n', score[0])\r\n",
        "  print(score[1])\r\n",
        "  scores.append(score[1])\r\n",
        "print('\\n', 'mean is: ', np.mean(scores))\r\n",
        "print('std is: ', np.std(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  app.launch_new_instance()\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:20<03:03, 20.39s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.08208760619163513\n",
            "0.9810189604759216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 2/10 [00:40<02:42, 20.34s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.08270856738090515\n",
            "0.9786773324012756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 30%|███       | 3/10 [01:00<02:22, 20.35s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.07617900520563126\n",
            "0.9825029373168945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 40%|████      | 4/10 [01:21<02:03, 20.52s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.10433650016784668\n",
            "0.9738333225250244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 50%|█████     | 5/10 [01:42<01:42, 20.54s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.08375798165798187\n",
            "0.9788333177566528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 60%|██████    | 6/10 [02:03<01:22, 20.61s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.08791996538639069\n",
            "0.9769961833953857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 70%|███████   | 7/10 [02:23<01:01, 20.58s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.0808865875005722\n",
            "0.9794965982437134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 80%|████████  | 8/10 [02:44<00:41, 20.61s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.08395864814519882\n",
            "0.9796632528305054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 90%|█████████ | 9/10 [03:04<00:20, 20.57s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.06538508087396622\n",
            "0.9814907312393188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 10/10 [03:25<00:00, 20.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.09501895308494568\n",
            "0.9743162393569946\n",
            "\n",
            " mean is:  0.9786828875541687\n",
            "std is:  0.0027388094618208485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}