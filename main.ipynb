{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "lToePHjXbGB1"
   },
   "outputs": [],
   "source": [
    "#Tensorflow and keras imports\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AvgPool2D, Flatten, Dense\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#Other imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "B_MbntNNZnZZ"
   },
   "outputs": [],
   "source": [
    "number_of_dataset_classes = 10\n",
    "number_of_K_folds = 10\n",
    "dataset = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "_SICsz20Rksv"
   },
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name):\n",
    "    if dataset_name == 'mnist':\n",
    "        (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    elif dataset_name == 'fashion_mnist':\n",
    "        (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "    return (X_train, Y_train), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ka5e_LGfY_lD"
   },
   "outputs": [],
   "source": [
    "def separate_dataset_into_K_folds(X_train, Y_train, number_of_K_folds):\n",
    "    if number_of_K_folds == 10:\n",
    "        folds = get_10_folds(X_train, Y_train)\n",
    "    elif number_of_K_folds == 5:\n",
    "        folds = get_5_folds(X_train, Y_train)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mW0wc7VoaRai"
   },
   "outputs": [],
   "source": [
    "def get_10_folds(X_train, Y_train, number_of_dataset_classes = number_of_dataset_classes):\n",
    "    dataset_classes = get_dataset_classes(X_train, Y_train, number_of_dataset_classes)\n",
    "    X_folds = [[],[],[],[],[],[],[],[],[],[]]\n",
    "    Y_folds = [[],[],[],[],[],[],[],[],[],[]]\n",
    "  \n",
    "  #Pick each dataset class\n",
    "  for jj, dataset_class in enumerate(dataset_classes):\n",
    "    image_index = 0\n",
    "    while image_index < len(dataset_class):\n",
    "        for ii, fold in enumerate(X_folds):\n",
    "          try:\n",
    "            #print('image index is: ', image_index, 'dataset_class_index is: ', dataset_class_index)\n",
    "            X_folds[ii].append(dataset_class[image_index])\n",
    "            Y_folds[ii].append(jj)\n",
    "            image_index += 1\n",
    "          except Exception as e:\n",
    "            continue\n",
    "        \n",
    "  #Convert X_folds and Y_folds to numpy arrays\n",
    "  for ii, fold in enumerate(X_folds):\n",
    "    X_folds[ii] = np.array(fold)\n",
    "  for ii, fold in enumerate(Y_folds):\n",
    "    Y_folds[ii] = np.array(fold)\n",
    "  X_folds = np.array(X_folds)\n",
    "  Y_folds = np.array(Y_folds)\n",
    "\n",
    "  for ii, X_fold in enumerate(X_folds):\n",
    "    Y_fold = Y_folds[ii]\n",
    "    #c = np.array([X_fold, Y_fold])\n",
    "    indices = np.arange(X_fold.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    # c[0] = c[0][indices]\n",
    "    # c[1] = c[1][indices]\n",
    "    # X_fold, Y_fold = c[0], c[1]\n",
    "    X_fold = X_fold[indices]\n",
    "    Y_fold = Y_fold[indices]\n",
    "    X_folds[ii] = X_fold\n",
    "    Y_folds[ii] = Y_fold\n",
    "\n",
    "  return X_folds, Y_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjJ6daV0Pfu8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qzw9kLSFbKIn"
   },
   "outputs": [],
   "source": [
    "def get_dataset_classes(X_train, Y_train, number_of_dataset_classes):\n",
    "    if number_of_dataset_classes == 10:\n",
    "        dataset_classes = [[],[],[],[],[],[],[],[],[],[]]\n",
    "    elif number_of_dataset_classes == 5:\n",
    "        dataset_classes = [[],[],[],[],[]]\n",
    "    for dataset_class_index in range(number_of_dataset_classes):\n",
    "        for item in range(X_train.shape[0]):\n",
    "            if Y_train[item] == dataset_class_index:\n",
    "                dataset_classes[dataset_class_index].append(X_train[item])\n",
    "    return np.array(dataset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TgevJCFke7Xp"
   },
   "outputs": [],
   "source": [
    "def create_fold_iterables(X_folds, Y_folds):\n",
    "    iterables = []\n",
    "    for ii, val_fold in enumerate(X_folds):\n",
    "        X_stack = 0\n",
    "        Y_stack = 0\n",
    "        for jj, train_fold in enumerate(X_folds):\n",
    "          if ii != jj:\n",
    "            if type(X_stack) is int:\n",
    "                X_stack = train_fold\n",
    "                Y_stack = Y_folds[jj]\n",
    "            else:\n",
    "                X_stack= np.vstack((X_stack, train_fold))\n",
    "                Y_stack= np.hstack((Y_stack, Y_folds[jj]))\n",
    "\n",
    "    iterables.append([X_stack, Y_stack, val_fold, Y_folds[ii]])\n",
    "    iterables = np.array(iterables)\n",
    "    return iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wcty3QInZPh_"
   },
   "outputs": [],
   "source": [
    "#https://medium.com/towards-artificial-intelligence/the-architecture-implementation-of-lenet-5-eef03a68d1f7\n",
    "def create_lenet5_model_with_1_conv_layers():\n",
    "    \n",
    "    # Instanciate an empty model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adding a Convolution Layer C1\n",
    "    # Input shape = N = (28 x 28)\n",
    "    # No. of filters  = 6\n",
    "    # Filter size = f = (5 x 5)\n",
    "    # Padding = P = 0\n",
    "    # Strides = S = 1\n",
    "    # Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24\n",
    "    # No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156\n",
    "    model.add(Conv2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=(28,28,1), activation='tanh'))\n",
    "    \n",
    "    # Adding an Average Pooling Layer S2\n",
    "    # Input shape = N = (24 x 24)\n",
    "    # No. of filters = 6\n",
    "    # Filter size = f = (2 x 2)\n",
    "    # Padding = P = 0\n",
    "    # Strides = S = 2\n",
    "    # Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12\n",
    "    # No. of parameters between C1 and S2 = (1+1)*6 = 12\n",
    "    model.add(AvgPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Adding a Convolution Layer C3\n",
    "    # Input shape = N = (12 x 12)\n",
    "    # No. of filters  = 16\n",
    "    # Filter size = f = (5 x 5)\n",
    "    # Padding = P = 0\n",
    "    # Strides = S = 1\n",
    "    # Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8\n",
    "    # No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416\n",
    "    model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
    "    \n",
    "    # Adding an Average Pooling Layer S4\n",
    "    # Input shape = N = (8 x 8)\n",
    "    # No. of filters = 16\n",
    "    # Filter size = f = (2 x 2)\n",
    "    # Padding = P = 0\n",
    "    # Strides = S = 2\n",
    "    # Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4\n",
    "    # No. of parameters between C3 and S4 = (1+1)*16 = 32\n",
    "    model.add(AvgPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    # As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of \n",
    "    # convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more\n",
    "    # convolution here.\n",
    "    \n",
    "    # Flattening the layer S4\n",
    "    # There would be 16*(4*4) = 256 neurons\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Adding a Dense layer with `tanh` activation+# \n",
    "    # No. of inputs = 256\n",
    "    # No. of outputs = 120\n",
    "    # No. of parameters = 256*120 + 120 = 30,840\n",
    "    model.add(Dense(120, activation='tanh'))\n",
    "    \n",
    "    # Adding a Dense layer with `tanh` activation\n",
    "    # No. of inputs = 120\n",
    "    # No. of outputs = 84\n",
    "    # No. of parameters = 120*84 + 84 = 10,164\n",
    "    model.add(Dense(84, activation='tanh'))\n",
    "    \n",
    "    # Adding a Dense layer with `softmax` activation\n",
    "    # No. of inputs = 84\n",
    "    # No. of outputs = 10\n",
    "    # No. of parameters = 84*10 + 10 = 850\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "_FyhtcyvAuS2"
   },
   "outputs": [],
   "source": [
    "def compile_and_fit_model(model, train_x, train_y, opt, lr):#, val_x, val_y):\n",
    "    #Reshape data\n",
    "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
    "    #val_x = val_x.reshape(val_x.shape[0], 28, 28, 1)\n",
    "    \n",
    "    #Normalize data\n",
    "    train_x = train_x/255.0\n",
    "    #val_x = val_x/255.0\n",
    "    \n",
    "    #One-hot encode the labels\n",
    "    train_y = to_categorical(train_y, num_classes=10)\n",
    "    #print('train_y.shape is: ', train_y.shape)\n",
    "    #val_y = to_categorical(val_y, num_classes=10)\n",
    "    \n",
    "    if opt == 'Adam':\n",
    "        opt = Adam(learning_rate = lr)\n",
    "    elif opt == 'SGD':\n",
    "        opt = SGD(learning_rate = lr)\n",
    "    elif opt == 'RMSprop':\n",
    "        opt = RMSprop(learning_rate = lr)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.fit(train_x, train_y, batch_size=128, epochs=20, verbose=0)#, validation_data=(val_x, val_y))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "gGIMJOw0A5bN"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_x, test_y):\n",
    "    #val_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
    "    print('val_x.shape: ', val_x.shape)\n",
    "    val_x = test_x/255.0\n",
    "    val_y = to_categorical(test_y, num_classes=10)\n",
    "    print('val_y.shape: ', val_y.shape)\n",
    "    score = model.evaluate(val_x, val_y, batch_size=128)\n",
    "    print('Test Loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_e_nZYpYQPu",
    "outputId": "8f1ab14c-27e1-4b82-c011-114b5b2bf125"
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh36RHoKWD-h",
    "outputId": "85f4d563-1d01-4d03-ff2e-8c751b50e14f"
   },
   "outputs": [],
   "source": [
    "X_folds, Y_folds = separate_dataset_into_K_folds(X_train, Y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "architectures = ['1ConvLayer', '2ConvLayers','3ConvLayers']\n",
    "optimizers = ['Adam', 'SGD', 'RMSprop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVOTNPBUUt3v",
    "outputId": "26e78237-b4e2-420f-f776-e33ae9a11aeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:34<00:00, 27.45s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam 0.1\n",
      "\n",
      " mean is:  0.10241641849279404\n",
      "std is:  0.007112549819412418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:17<00:00, 25.74s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD 0.1\n",
      "\n",
      " mean is:  0.9747996211051941\n",
      "std is:  0.011253604242158986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:29<00:00, 32.92s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSprop 0.1\n",
      "\n",
      " mean is:  0.09759966507554055\n",
      "std is:  0.004190999928751982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:30<00:00, 27.07s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam 0.01\n",
      "\n",
      " mean is:  0.971066826581955\n",
      "std is:  0.0033341250129321136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:58<02:58, 29.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-dc006a553f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_fold_Y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lenet5_model_with_1_conv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_fold_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_fold_Y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m           \u001b[0;31m#evaluate_model(model, K_fold_X_test, K_fold_Y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-03e34c9750cd>\u001b[0m in \u001b[0;36mcompile_and_fit_model\u001b[0;34m(model, train_x, train_y, opt, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, validation_data=(val_x, val_y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterables = create_fold_iterables(X_folds, Y_folds)\n",
    "for lr in learning_rates:\n",
    "    #for arch in architectures:\n",
    "    for opt in optimizers:\n",
    "        scores = []\n",
    "        for iterable in tqdm(iterables):\n",
    "            K_fold_X_train, K_fold_Y_train, K_fold_X_test, K_fold_Y_test = iterable\n",
    "            # print(K_fold_X_train.shape)\n",
    "            # print(K_fold_Y_train.shape)\n",
    "            # print(K_fold_X_test.shape)\n",
    "            # print(K_fold_Y_test.shape)\n",
    "            val_x = K_fold_X_test.reshape(K_fold_X_test.shape[0], 28, 28, 1)\n",
    "            val_y = to_categorical(K_fold_Y_test, num_classes=10)\n",
    "            model = create_lenet5_model_with_1_conv_layers()\n",
    "            model = compile_and_fit_model(model, K_fold_X_train, K_fold_Y_train, opt, lr)\n",
    "            #evaluate_model(model, K_fold_X_test, K_fold_Y_test)\n",
    "            score = model.evaluate(val_x, val_y, verbose=0)\n",
    "            #print('\\n', score[0])\n",
    "            #print(score[1])\n",
    "            scores.append(score[1])\n",
    "        print(opt, lr)\n",
    "        print('\\n', 'mean is: ', np.mean(scores))\n",
    "        print('std is: ', np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

"""
def create_lenet5_model_with_2_conv_layers():
    
    # Instanciate an empty model
    model = Sequential()
    
    # Adding a Convolution Layer C1
    # Input shape = N = (28 x 28)
    # No. of filters  = 6
    # Filter size = f = (5 x 5)
    # Padding = P = 0
    # Strides = S = 1
    # Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24
    # No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156
    model.add(Conv2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=(28,28,1), activation='relu'))
    
    # Adding an Average Pooling Layer S2
    # Input shape = N = (24 x 24)
    # No. of filters = 6
    # Filter size = f = (2 x 2)
    # Padding = P = 0
    # Strides = S = 2
    # Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12
    # No. of parameters between C1 and S2 = (1+1)*6 = 12
    model.add(AvgPool2D(pool_size=(2,2)))
    
    # Adding a Convolution Layer C3
    # Input shape = N = (12 x 12)
    # No. of filters  = 16
    # Filter size = f = (5 x 5)
    # Padding = P = 0
    # Strides = S = 1
    # Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8
    # No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416
    model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu'))
    
    # Adding an Average Pooling Layer S4
    # Input shape = N = (8 x 8)
    # No. of filters = 16
    # Filter size = f = (2 x 2)
    # Padding = P = 0
    # Strides = S = 2
    # Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4
    # No. of parameters between C3 and S4 = (1+1)*16 = 32
    model.add(AvgPool2D(pool_size=(2,2)))
    
    # As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of 
    # convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more
    # convolution here.
    
    # Flattening the layer S4
    # There would be 16*(4*4) = 256 neurons
    model.add(Flatten())
    
    # Adding a Dense layer with `tanh` activation+# 
    # No. of inputs = 256
    # No. of outputs = 120
    # No. of parameters = 256*120 + 120 = 30,840
    #model.add(Dense(120, activation='relu'))
    
    # Adding a Dense layer with `tanh` activation
    # No. of inputs = 120
    # No. of outputs = 84
    # No. of parameters = 120*84 + 84 = 10,164
    model.add(Dense(84, activation='relu'))
    
    # Adding a Dense layer with `softmax` activation
    # No. of inputs = 84
    # No. of outputs = 10"
    # No. of parameters = 84*10 + 10 = 850
    model.add(Dense(10, activation='softmax'))
    
    print(model.summary())
    
    return model
"""
"""
def create_lenet5_model_with_3_conv_layers():
    
    # Instanciate an empty model
    model = Sequential()
    
    # Adding a Convolution Layer C1
    # Input shape = N = (28 x 28)
    # No. of filters  = 6
    # Filter size = f = (5 x 5)
    # Padding = P = 0
    # Strides = S = 1
    # Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24
    # No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156
    model.add(Conv2D(filters=6, kernel_size=(4,4), padding='valid', input_shape=(28,28,1), activation='relu'))
    
    # Adding an Average Pooling Layer S2
    # Input shape = N = (24 x 24)
    # No. of filters = 6
    # Filter size = f = (2 x 2)
    # Padding = P = 0
    # Strides = S = 2
    # Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12
    # No. of parameters between C1 and S2 = (1+1)*6 = 12
    model.add(AvgPool2D(pool_size=(2,2)))
    
    # Adding a Convolution Layer C3
    # Input shape = N = (12 x 12)
    # No. of filters  = 16
    # Filter size = f = (5 x 5)
    # Padding = P = 0
    # Strides = S = 1
    # Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8
    # No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416
    model.add(Conv2D(filters=16, kernel_size=(4,4), padding='valid', activation='relu'))
    
    # Adding an Average Pooling Layer S4
    # Input shape = N = (8 x 8)
    # No. of filters = 16
    # Filter size = f = (2 x 2)
    # Padding = P = 0
    # Strides = S = 2
    # Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4
    # No. of parameters between C3 and S4 = (1+1)*16 = 32
    model.add(AvgPool2D(pool_size=(2,2)))

    # Adding a Convolution Layer
    model.add(Conv2D(filters=32, kernel_size=(2,2), padding='valid', activation='relu'))
    
    # Adding an Average Pooling Layer
    model.add(AvgPool2D(pool_size=(2,2)))
    
    # As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of 
    # convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more
    # convolution here.
    
    # Flattening the layer S4
    # There would be 16*(4*4) = 256 neurons
    model.add(Flatten())
    
    # Adding a Dense layer with `tanh` activation+# 
    # No. of inputs = 256
    # No. of outputs = 120
    # No. of parameters = 256*120 + 120 = 30,840
    #model.add(Dense(120, activation='relu'))
    
    # Adding a Dense layer with `tanh` activation
    # No. of inputs = 120
    # No. of outputs = 84
    # No. of parameters = 120*84 + 84 = 10,164
    model.add(Dense(84, activation='relu'))
    
    # Adding a Dense layer with `softmax` activation
    # No. of inputs = 84
    # No. of outputs = 10
    # No. of parameters = 84*10 + 10 = 850
    model.add(Dense(10, activation='softmax'))
    
    print(model.summary())
    
    return model
"""
"""
def create_lenet5_model_with_4_conv_layers():
    
    # Instanciate an empty model
    model = Sequential()
    
    # Adding a Convolution Layer C1
    # Input shape = N = (28 x 28)
    # No. of filters  = 6
    # Filter size = f = (5 x 5)
    # Padding = P = 0
    # Strides = S = 1
    # Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24
    # No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156
    model.add(Conv2D(filters=6, kernel_size=(2,2), padding='valid', input_shape=(28,28,1), activation='relu'))
    
    # Adding an Average Pooling Layer S2
    # Input shape = N = (24 x 24)
    # No. of filters = 6
    # Filter size = f = (2 x 2)
    # Padding = P = 0
    # Strides = S = 2
    # Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12
    # No. of parameters between C1 and S2 = (1+1)*6 = 12
    model.add(AvgPool2D(pool_size=(2,2)))
    
    # Adding a Convolution Layer C3
    # Input shape = N = (12 x 12)
    # No. of filters  = 16
    # Filter size = f = (5 x 5)
    # Padding = P = 0
    # Strides = S = 1
    # Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8
    # No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416
    model.add(Conv2D(filters=16, kernel_size=(2,2), padding='valid', activation='relu'))
    
    # Adding an Average Pooling Layer S4
    # Input shape = N = (8 x 8)
    # No. of filters = 16
    # Filter size = f = (2 x 2)
    # Padding = P = 0
    # Strides = S = 2
    # Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4
    # No. of parameters between C3 and S4 = (1+1)*16 = 32
    model.add(AvgPool2D(pool_size=(2,2)))

    # Adding a Convolution Layer
    model.add(Conv2D(filters=32, kernel_size=(2,2), padding='valid', activation='relu'))
    
    # Adding an Average Pooling Layer
    model.add(AvgPool2D(pool_size=(2,2)))

    # Adding a Convolution Layer
    model.add(Conv2D(filters=32, kernel_size=(2,2), padding='valid', activation='relu'))
    
    # Adding an Average Pooling Layer
    #model.add(AvgPool2D(pool_size=(2,2)))
    
    # As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of 
    # convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more
    # convolution here.
    
    # Flattening the layer S4
    # There would be 16*(4*4) = 256 neurons
    model.add(Flatten())
    
    # Adding a Dense layer with `tanh` activation+# 
    # No. of inputs = 256
    # No. of outputs = 120
    # No. of parameters = 256*120 + 120 = 30,840
    #model.add(Dense(120, activation='relu'))
    
    # Adding a Dense layer with `tanh` activation
    # No. of inputs = 120
    # No. of outputs = 84
    # No. of parameters = 120*84 + 84 = 10,164
    model.add(Dense(84, activation='relu'))
    
    # Adding a Dense layer with `softmax` activation
    # No. of inputs = 84
    # No. of outputs = 10
    # No. of parameters = 84*10 + 10 = 850
    model.add(Dense(10, activation='softmax'))
    
    print(model.summary())
    
    return model
"""
